---
title: "Session 2: Pop Gen In Conservation"

project:
  execute-dir: project
---

# Pop Gen In Conservation

*Session Presenters*

![](images/Presenters2.png)

## *Required packages*

```{r setup, include = FALSE}
library(knitr)
library(formatR)
library(tidyverse)
library(gifski)
knitr::opts_chunk$set(tidy = TRUE, tidy.opts = list(width.cutoff = 72), 
                      cache =TRUE, 
                      echo = TRUE)

```

```{r, warning=FALSE, message=FALSE}
#BiocManager::install("LEA")
#devtools::install_github("tdhock/directlabels")

library(dartRverse)

library(ggplot2)
library(data.table)

library(leaflet.minicharts)
library(LEA)
```

*make sure you have the packages installed, see* [Install dartRverse](install.qmd)

## Introduction

Here is an introductory video on genetic diversity measures that is assumed knowledge for this session.

![](data/Sherwin_TREE_Box_1_Tutorial_170920.mp4)

For more details see @sherwin_information_2017.

## Exercise

::: callout-note
## Task

![](images/task.png){.class width="48" height="48"} You have been asked to evaluate the 'genetic health' of a population at a restoration site, which has been established two years earlier as a results of the drying of a wet land due to urban development. The corporate responsible for the restoration project claims that the area can now sustain a high density population of the species *A*, which previously only occurred in the forested areas west and east of the previously existing wet land. Species *A* is a long lived terrestrial animals.

Compute genetic diversity metrics (e.g. $He$, $Ho$, $Fis$) with the data generated by the 100 samples collected and provide a recommendation on whether the restoration project was successful and whether the studied population is likely to required continued active management in the short term.
:::

### Load data

```{r, warning=FALSE, message=FALSE, include=FALSE}
# Load the data
glsim <- gl.load("./data/sess2_glsim.rds")
```

```{r, eval = FALSE}
# Load the data
glsim <- gl.load("./data/sess2_glsim.rds")
```

```{r}
#check the population size and number of populations
table(pop(glsim))
#number of loci
nLoc(glsim)

```

## **Different dynamics, same result**

Several descriptive genetic parameters are almost routinely computed, and one would be tempted to interpret them with a standardised approach. For example, low genetic diversity is often considered as an indication that the population size is small. If the inbreeding coefficient ($Fis$) is positive, that is often taken as an indication that the population is inbred and potentially at risk of inbreeding depression. However, there are situation where different dynamics can produce the same result. It is not always easy to diagnose these situations and identify the cause of our results, but my recommendation would be to give priority to your understanding of the biology, ecology and history of the populations and species you are working with. If the suggested interpretation do not fit your understanding of the ecology or demographics of the populations you are working with, explore possible alternative explanations. In the example above, a low genetic diversity can also occur because of a severe bottleneck even if the population has then recovered to large numbers.

Recall that $Fis = (He-Ho)/He$, hence any cause of alteration of HWE can cause an increased/decrease $Fis$ as well sampling effects. For example, the selective sampling of related individuals (e.g. when sampling individuals within family groups - as often happens with feral pigs/boar - or when you are sampling intensively within the dispersal distance range of few individuals) can cause your samples not to be representative of the population, and while your analysis could indicate inbreeding, this may not be the case at population level. Wahlund effect is also another common cause of increased $Fis$.

Let's have a look at an example of what happens if our sampling design caused us to sample selectively related individuals. We are going to use a simulated dataset based on the concept presented in Pacioni et al (2020) where females intensively trapped (using a grid design, 'G') within an area that didn't encompassed multiple home ranges were highly related. ON the contrary, individuals trapped along transect ('T') that extended over multiple home ranges were an actual random rapresentation of the population.

### Load data

```{r, include=FALSE}
glw <- gl.load("./data/sess2_glDes.rds")
#check the population size and number of populations
table(pop(glw)) # G stands for trapped in a grid, T for transect
#number of loci
nLoc(glw)

glw <- gl.filter.monomorphs(glw, verbose = 5)
```

```{r, eval = FALSE}
glw <- gl.load("./data/sess2_glDes.rds")
#check the population size and number of populations
table(pop(glw)) # G stands for trapped in a grid, T for transect
#number of loci
nLoc(glw)

glw <- gl.filter.monomorphs(glw, verbose = 5)
```

### Calculate Relatedness

```{r}
# Run EMIBD9 to detect related individuals. Output saved to save time
#ibd9 <- gl.run.EMIBD9(glw, Inbreed = FALSE, 
 #                     emibd9.path = "E:/Software/Genetic software/EMIBD9")
#saveRDS(ibd9, file="Gen Diversity/ibd9.rds")

# Load EMIBD9 results
ibd9 <- readRDS("./data/sess2_ibd9.rds")

# Do some manipulation to have everything in one table
ibd9Tab <- ibd9[[2]]
# Kick out self comparisons
ibd9Tab <- ibd9Tab[ibd9Tab$Indiv1 != ibd9Tab$Indiv2,  c(1, 2, 21)]
# Add trapping des for each individuals
Des1 <- pop(glw)[as.numeric(ibd9Tab$Indiv1)]
Des2 <- pop(glw)[as.numeric(ibd9Tab$Indiv2)]
# Flag pairs trapping within G, T and in between (BW)
PDes <- ifelse((Des1 == "G" & Des2 == "G"), yes = "G", 
       no = ifelse((Des1 == "T" & Des2 == "T"), yes = "T", "BW"))
# Combine together
ibd9DT <- data.table(cbind(ibd9Tab, Des1, Des2, PDes))
# Compute the mean relatedness
ibd9DT[, mean(as.numeric(`r(1,2)`)), by=PDes]

```

### Relatedness results

Mean relatedness for animals trapped in the grid is 3 times those trapped with a transect. There are also a series of other interesting things that happens: Mean relatedness of animals trapped in the grid is just marginally above the mean expected for first cousins (\~0.06). However, more than half of the comparisons were between half sibs or more related individuals:

```{r}
ibd9DT[, sum(`r(1,2)`>=0.125), by=PDes] # HS or more
ibd9DT[, sum(`r(1,2)`>=0.25), by=PDes] # FS or PO

# Recall that the number of pairwise comparisons within 'G' is
nG <- sum(pop(glw) == "G")
nG*(nG-1)/2
```

Only a limited number of loci are out of HWE after correction for multiple comparisons

```{r}
hwe <- gl.report.hwe(glw, multi_comp = TRUE)

```

But incredibly, because of the het excess, $Fis$ in 'G' is negative suggesting a outbred population!

```{r}
het <- gl.report.heterozygosity(glw)
```

::: callout-tip
### Interpreting results

In our task, when the restoration project took place, two separate populations came in contact, creating an admixture zone. As the species is long-lived there has not been enough mixing and turnover for the two amalgamate. The lack of HWE and increased $Fis$ is a result of the Wahlund effect.
:::

## Exercise data

```{r}
gl.report.heterozygosity(glsim)
pcaglsim <- gl.pcoa(glsim)
gl.pcoa.plot(glPca = pcaglsim, x = glsim)

hwe <- gl.report.hwe(glsim, multi_comp = TRUE)

```

## **Assessing Populations: structure and demographic history**

### Load data and explore

This dataset is to assess population structure and diversity in Uperoleia crassa, from @jaya_population_2022.

```{r}
# gl <- dartR::gl.read.dart("Report_DUp20-4995_1_moreOrders_SNP_mapping_2.csv",
#                           ind.metafile = "Uperoleia_metadata.csv")

load('./data/session_2.RData') # data named gl

```

These are monsoonal tropical frogs, who are very common and abundant. They do interesting and weird stuff evolutionarily and reproductively

lets quickly look at our samples and populations

```{r}
gl.map.interactive(gl)
```

we have two populations - the Kimberley (IK) and the Top End (IT)

### Clean data

Clean up your dataset to remove the most egregiously bad loci and individuals this set of filtering can be used across analyses

```{r}
#Get rid of really poorly sequenced loci
#But don’t cut hard
gl.report.callrate(gl)
gl.1 <- gl.filter.callrate(gl, method = "loc", threshold = 0.8)

#Very low filter – this is only to get rid of your really bad individuals
gl.report.callrate(gl.1, method = "ind")
gl.2 <- gl.filter.callrate(gl.1, method = "ind", threshold = 0.25)

#Always run this after removing individuals – removes loci that are no longer variable
gl.3 <- gl.filter.monomorphs(gl.2)

#Get rid of unreliable loci
gl.report.reproducibility(gl.3)
gl.4 <- gl.filter.reproducibility(gl.3)

#Get rid of low and super high read depth loci
#do twice so you can zoom in
gl.report.rdepth(gl.4)
gl.5 <- gl.filter.rdepth(gl.4, lower = 0, upper = 25)
gl.clean <- gl.filter.rdepth(gl.5, lower = 8, upper = 17)

nInd(gl.clean)
nLoc(gl.clean)

#look at the data to see if you see any obvious issues and redo if you do.
plot(gl.clean)

rm(gl.1, gl.2, gl.3, gl.4, gl.5)



```

::: callout-tip
## gl.clean

This file is now your starting file for other filtering
:::

### Filtering

##### **Filtering data for population structure and phylogenetic analyses**

loci on the same fragment and missing data are not well supported in these analyses. Structure-like analyses dislikes missing data. So, we are going to filter even harder than normal here just so that we can see a result.

```{r, output = FALSE}
gl.report.callrate(gl.clean)
gl.1 <- gl.filter.callrate(gl.clean, method = "loc", threshold = 0.98)

#Remove minor alleles
#I usually set up the threshold so it is just 
# removing singletons to improve computation time
gl.report.maf(gl.1)
gl.2 <- gl.filter.maf(gl.1,  threshold = 1/(2*nInd(gl.1)))

#check that the data looks fairly clean
#this starts to show some obvious population banding
plot(gl.2)

#remove secondary SNPs on the same fragment
#Always do this as the last loci filter so that you’ve cut for quality 
# before you cut because there are two SNPs
gl.3 <- gl.filter.secondaries(gl.2)

#Filter on individuals. You can usually be a bit flexible at this point.
#individuals look a whole lot better now
#make note of any idnviduals with a low call rate. Keep them in for now
#but if they act weird in the analysis, you may want to consider removing
gl.report.callrate(gl.3, method = "ind")
gl.4 <- gl.filter.callrate(gl.3, method = "ind", threshold = .9)
#Always run this after removing individuals
gl.structure <- gl.filter.monomorphs(gl.4)

#this is your cleaned dataset for a population structure analysis              
plot(gl.structure)

nInd(gl.structure)
nLoc(gl.structure)

```

you can write this file out for various analyses that we will not go into here (e.g. `gl2structure(x)` or `gl2faststructure(x)`)

### Run SNMF (LEA)

This is a structure-like analysis called SNMF. This code is not all you would need to publish this result, but it is a good first look.

```{r, output = FALSE}

#takes quite a while to run (about 15 minutes)
#in case you want to run it uncomment the following lines and comment the readRDS line

# LEA requires the genotype style file
#gl2faststructure(gl.structure, outfile = "gl_structure.fstr",
#                 outpath = './data/')
#struct2geno("./data/gl_structure.fstr", ploidy = 2, FORMAT = 2) 
###this hates any loci with all heterozygotes

#snmf.Sy.K1_8.10 = snmf("./data/gl_structure.fstr.geno", K = 1:8,
#                       entropy = T, ploidy = 2, project="new", repetitions = 10)

snmf.Sy.K1_8.10 <- readRDS("./data/snmf.Sy.K1_8.10.rds")

```

```{r}
plot(snmf.Sy.K1_8.10)
k <- 2 #chose best based on lowest cross entropy in graph
ce = cross.entropy(snmf.Sy.K1_8.10, K = k)
best <- which.min(ce)
par (mfrow = c(1,1))
barplot(t(Q(snmf.Sy.K1_8.10, K = k, run = best)), col = 1:k)


#Do a PCoA plot. I had these but they are also good for some stuff
pc <- gl.pcoa(gl.structure)
gl.pcoa.plot(pc, gl.structure)

rm(ce, gl.1, gl.2, gl.3, gl.4, snmf.Sy.K1_8.10, best, k, pc)


```

::: callout-note
## Exercise

![](images/task.png){#id .class width="48" height="48"} Have a go at altering various parameters and seeing how this changes your answers.

One thing that regularly changes with MAF filters is the amount of variance explained. Removing more minor alleles increases the variance explained in a PCoA. Think about why that would be the case.
:::

### Filtering for Tajima's D

Now we are going to look at how filtering can affect your understanding of demographic processes that population is Significant negative values of Tajima's D are due to an excess of rare alleles and are consistent with range expansion. Significant positive values are associated with population contraction. Significance (a P-value) cannot be calculated without a simulation. Please ask me how to do this using Hudson's ms program. Rare alleles matter! Lets look at how singletons impact our estimations of whether a population is expanding or contracting

This is a function written to calculate Tajima's D from SNP data. It will create the function in your global environment so that you can call it.

```{r}
get_tajima_D <- function(x){
  require(dartRverse) # possibly not needed for a function in an R package?
  
  # Find allele frequencies (p1 and p2) for every locus in every population
  allele_freqs <- gl.allele.freq(x)
  names(allele_freqs)[names(allele_freqs) == "frequency"] <- "p1"
  allele_freqs$p1 <- allele_freqs$p1 / 100
  allele_freqs$p2 <- 1 - allele_freqs$p1
  
  # Get the names of all the populations
  pops <- unique(allele_freqs$popn)
  
  #split each population
  allele_freqs_by_pop <- split(allele_freqs, allele_freqs$popn)
  
  # Internal function to calculate pi
  calc_pi <- function(allele_freqs) {
    n = allele_freqs$nobs * 2  # vector of n values
    pi_sqr <- allele_freqs$p1 ^ 2 + allele_freqs$p2 ^ 2
    h = (n / (n - 1)) * (1 - pi_sqr) # vector of values of h
    sum(h) # return pi, which is the sum of h across loci
  }
  
  get_tajima_D_for_one_pop <- function(allele_freqs_by_pop) {
    pi <- calc_pi(allele_freqs_by_pop)
    
# Calculate number of segregating sites, ignoring missing data (missing data will not appear in teh allele freq calcualtions)
    #S <- sum(!(allele_freqs_by_pop$p1 == 0 | allele_freqs_by_pop$p1 == 1))
    S <- sum(allele_freqs_by_pop$p1 >0 & allele_freqs_by_pop$p1 <1)
    if(S == 0) {
      warning("No segregating sites")
      data.frame(pi = NaN, 
                 S = NaN, 
                 D = NaN, 
                 Pval.normal = NaN, 
                 Pval.beta = NaN)
    }
    
    n <- mean(allele_freqs_by_pop$nobs * 2 )
    
    tmp <- 1:(n - 1)
    a1 <- sum(1/tmp)
    a2 <- sum(1/tmp^2)
    b1 <- (n + 1)/(3 * (n - 1))
    b2 <- 2 * (n^2 + n + 3)/(9 * n * (n - 1))
    c1 <- b1 - 1/a1
    c2 <- b2 - (n + 2)/(a1 * n) + a2/a1^2
    e1 <- c1/a1
    e2 <- c2/(a1^2 + a2)
    
    
    #calculate D and do beta testing
    D <- (pi - S/a1) / sqrt(e1 * S + e2 * S * (S - 1))
    Dmin <- (2/n - 1/a1)/sqrt(e2)
    Dmax <- ((n/(2*(n - 1))) - 1/a1)/sqrt(e2)
    tmp1 <- 1 + Dmin * Dmax
    tmp2 <- Dmax - Dmin
    a <- -tmp1 * Dmax/tmp2
    b <- tmp1 * Dmin/tmp2
  
    
    data.frame(pi = pi, 
               S = S, 
               D = D)
  }
  
  output <- do.call("rbind", lapply(allele_freqs_by_pop, 
                                    get_tajima_D_for_one_pop))
  data.frame(population = rownames(output), output, row.names = NULL)
}

```

We are going to focus on key filters and how they impact estimates

1.  Minor allele frequency (MAF) filtering
2.  No MAF allele filtering
3.  No MAF filtering but filtering on Read depth

#### 1. **MAF filtering**

We will start with our lightly cleaned data and filter this for the tajima's calculation

```{r}

#This function is written to calculate Tajima's D with a fair amount of missing data
#we are going to filter lightly here 
gl.report.callrate(gl.clean)
gl.1 <- gl.filter.callrate(gl.clean, method = "loc", threshold = 0.9)

#In this first round, we are going to actually remove singletons to see what happens
gl.report.maf(gl.1)
nLoc(gl.1)
gl.2 <- gl.filter.maf(gl.1,  threshold = 0.05)
nLoc(gl.2)

#check that the data looks fairly clean
#this starts ot show some obvious population banding
plot(gl.2)

#we are also going to remove secondary SNPs on the same fragment in this first round
gl.3 <- gl.filter.secondaries(gl.2)

#Filter on individuals. You can usually be a bit flexible at this point.
#make note of any idnviduals with a low call rate. Keep them in for now
#but if they act weird in the analysis, you may want to consider removing
gl.report.callrate(gl.3, method = "ind")
gl.4 <- gl.filter.callrate(gl.3, method = "ind", threshold = .9)
#Always run this after removing individuals
gl.D_withfiltering <- gl.filter.monomorphs(gl.4)


#calculate tajima's D with removing singletons and secondaries

D_w_filtering <- get_tajima_D(gl.D_withfiltering)
D_w_filtering

rm(gl.1, gl.2, gl.3, gl.4)

```

#### 2. **No MAF filtering**

Now lets try it where we *don't* remove singletons or secondaries

```{r}
#This function is written to calculate Tajima's D with a fair amount of missing data
#we are going to filter lightly here 
gl.report.callrate(gl.clean)
gl.1 <- gl.filter.callrate(gl.clean, method = "loc", threshold = 0.9)


#check that the data looks fairly clean
#this starts ot show some obvious population banding
plot(gl.1)

#Filter on individuals. You can usually be a bit flexible at this point.
#make note of any idnviduals with a low call rate. Keep them in for now
#but if they act weird in the analysis, you may want to consider removing
gl.report.callrate(gl.1, method = "ind")
gl.2 <- gl.filter.callrate(gl.1, method = "ind", threshold = .9)
#Always run this after removing individuals
gl.D_withOutfiltering <- gl.filter.monomorphs(gl.2)

rm(gl.1, gl.2)

D_wOUT_filtering <- get_tajima_D(gl.D_withOutfiltering)

```

#### 3. **No MAF filtering but filtering on Read depth**

Now lets try it where we *don't* remove singletons or secondaries AND we filter for read depth to remove singletons where we are not confident about their base calls

```{r}
#This function is written to calculate Tajima's D with a fair amount of missing data
# we are going to filter lightly here 
gl.report.callrate(gl.clean)
gl.1 <- gl.filter.callrate(gl.clean, method = "loc", threshold = 0.9)


#check that the data looks fairly clean
#this starts ot show some obvious population banding
plot(gl.1)

#filter to loci with a lower read depth, so that we are really confident
#that our base calls are correct
gl.report.rdepth(gl.1)
gl.2 <- gl.filter.rdepth(gl.1, lower = 12, upper = 17)


#Filter on individuals. You can usually be a bit flexible at this point.
#make note of any idnviduals with a low call rate. Keep them in for now
#but if they act weird in the analysis, you may want to consider removing
gl.report.callrate(gl.2, method = "ind")
gl.3 <- gl.filter.callrate(gl.2, method = "ind", threshold = .9)
#Always run this after removing individuals
gl.D_withOutfilteringRDepth <- gl.filter.monomorphs(gl.3)

rm(gl.1, gl.2, gl.3)

#calculate tajima's D with removing singletons and secondaries

D_wOUT_filtering_Rdepth <- get_tajima_D(gl.D_withOutfilteringRDepth)

```

### Results

##### *lets look at all three*

```{r}
D_w_filtering
D_wOUT_filtering
D_wOUT_filtering_Rdepth
```

::: callout-caution
### Interpreting results

For our Kimberley population, removing minor alleles give the actual wrong answer and suggests the population is contracting. This would have contradicted the rest of the findings of the paper, and been an artifact of the filtering process.

This shows how understanding the population genetic theory for a metric is crucial to useful analyses.
:::

## *Further Study*

Another teaching app in R, covering only Shannon for genes and its change over time, produced by Computer Science and Engineering third-year students: <https://evolutionaryecology.shinyapps.io/learningEE>

### Readings

@oreilly_predicting_2020

@sherwin_bray-curtis_2022

@sherwin_information_2017

@sherwin_information_2021

@jaya_population_2022
